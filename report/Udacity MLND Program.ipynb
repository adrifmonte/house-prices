{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project\n",
    "\n",
    "Adriano Falsarella Monte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course Mentor\n",
    "\n",
    "Fernando Marcos Wittmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "[comment]: # (_approx. 1-2 pages_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview\n",
    "\n",
    "[comment]: # (In this section, look to provide a high-level overview of the project in layman’s terms. Questions to ask yourself when writing this section:)\n",
    "\n",
    "[comment]: # (Has an overview of the project been provided, such as the problem domain, project origin, and related datasets or input data?)\n",
    "[comment]: # (Has enough background information been given so that an uninformed reader would understand the problem domain and following problem statement?)\n",
    "\n",
    "What is a fair offer to place when selling or buying a home? How to efficiently come to that fair price? The real estate area has been adopting machine learning to analyse historical data with several criterias to find a well balanced model to help predicting house prices.\n",
    "\n",
    "After talking to Loft's CTO in a local event at Sao Paulo - Brazil, I've found out that [Loft uses machine learning to predict the house prices](https://blog.loft.com.br/entenda-como-a-loft-chega-no-valor-de-um-apartamento/) in their business, and data science is one of their core values. In order to exercise house prices predictions, let's use the [Kaggle's House Prices' getting started competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) public dataset, which provides an appropriate input for this project.\n",
    "\n",
    "To predict a house price, a model should be trained with historical containing the houses features and their corresponding house sold price. Despite that the houses in Ames - Iowa are different from the houses here in Brazil, the [dataset provided in Kaggle's House Prices' competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) still gets the essence of the work that would be done if the data was from a different location since most of the features would also be applicable in different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "[comment]: # (In this section, you will want to clearly define the problem that you are trying to solve, including the strategy {outline of tasks} you will use to achieve the desired solution. You should also thoroughly discuss what the intended solution will be for this problem. Questions to ask yourself when writing this section:)\n",
    "\n",
    "[comment]: # (Is the problem statement clearly defined? Will the reader understand what you are expecting to solve?)\n",
    "[comment]: # (Have you thoroughly discussed how you will attempt to solve the problem?)\n",
    "[comment]: # (Is an anticipated solution clearly defined? Will the reader understand what results you are looking for?)\n",
    "\n",
    "Given a dataset publicly available at [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) of homes in Ames - Iowa, having 79 house features, and containing 1460 entry points for training and 1459 entry points for testing, build the best model to predict house sale prices. A great model will predict the house prices as closest as possible to its actual labelled price.\n",
    "\n",
    "In the conversation I had with Loft's CTO, he told me that with the house's dataset, they make some clusters, and for each cluster they apply a separate regression. I love how it embraces both unsupervised and supervised learning to get a refined result, so the idea is to try it out, exercising both concepts in this Capstone Project.\n",
    "\n",
    "After clustering the data, find the best regression model for each cluster. With the trained models, to make a prediction, we need to first predict which cluster it is better represented, and then use that cluster's trained regression model to predict the house price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "[comment]: # (In this section, you will need to clearly define the metrics or calculations you will use to measure performance of a model or result in your project. These calculations and metrics should be justified based on the characteristics of the problem and problem domain. Questions to ask yourself when writing this section:)\n",
    "\n",
    "[comment]: # (Are the metrics you’ve chosen to measure the performance of your models clearly discussed and defined?)\n",
    "[comment]: # (Have you thoroughly discussed how you will attempt to solve the problem?)\n",
    "[comment]: # (Have you provided reasonable justification for the metrics chosen based on the problem and solution?)\n",
    "\n",
    "There are many evaluation methods available in scikit learn in the [`sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics) package. As per the proposed solution, we'll have to take one [clustering metric](https://scikit-learn.org/stable/modules/classes.html#clustering-metrics) and at one [regression metric](https://scikit-learn.org/stable/modules/classes.html#regression-metrics).\n",
    "\n",
    "For the clustering part, the project uses the silhouette score as the metric to find the best number of clusters, since it's the one that we've used most in the course. This evaluation method is provided by scikit learn in [`sklearn.metrics.silhouette_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) module.\n",
    "\n",
    "For the regression part, the project uses the root mean squared log error ([RMSLE](http://mkhalusova.github.io/blog/2019/04/17/ml-model-evaluation-metrics-p3#rmsle)) as the metric to find the best regression model, since it's the metric that is used as [evaluation](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation) in the Kaggle competition in the [Leaderboard ranking](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/leaderboard), but mainly because the squared error is aligned with the business objectives (we should predict prices with the least error possible between the prediction and the actual price). This evaluation method is also provided by scikit learn in [`sklearn.metrics.mean_squared_log_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html) module.\n",
    "\n",
    "To make all these measurements and processes replicable, the project always set a constant `random_state` when applicable. Also, a fixed and public dataset was chosen, [provided at Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data), and with a mirror in this [Project's repository](https://github.com/adrifmonte/house-prices) for both [train](https://github.com/adrifmonte/house-prices/blob/master/data/train.csv) and [test](https://github.com/adrifmonte/house-prices/blob/master/data/test.csv) data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "[comment]: # (_approx. 2-4 pages_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "[comment]: # (In this section, you will be expected to analyze the data you are using for the problem. This data can either be in the form of a dataset {or datasets}, input data {or input files}, or even an environment. The type of data should be thoroughly described and, if possible, have basic statistics and information presented {such as discussion of input features or defining characteristics about the input or environment}. Any abnormalities or interesting qualities about the data that may need to be addressed have been identified {such as features that need to be transformed or the possibility of outliers}. Questions to ask yourself when writing this section:)\n",
    "\n",
    "[comment]: # (If a dataset is present for this problem, have you thoroughly discussed certain features about the dataset? Has a data sample been provided to the reader?)\n",
    "[comment]: # (If a dataset is present for this problem, are statistics about the dataset calculated and reported? Have any relevant results from this calculation been discussed?)\n",
    "[comment]: # (If a dataset is *not* present for this problem, has discussion been made about the input space or input data for your problem?)\n",
    "[comment]: # (Are there any abnormalities or characteristics about the input space or dataset that need to be addressed? {categorical variables, missing values, outliers, etc.})\n",
    "\n",
    "There's a [brief description of each feature at Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data), as well as a [`txt` file with detailed data description, mirrored in this Project's repository](https://github.com/adrifmonte/house-prices/blob/master/data/data_description.txt) as well. Since there's lots of features, let me try to summarize them:\n",
    "\n",
    "The dataset is very feature-detailed, providing several characteristics about the dataset's houses, such as: the sale price in dollars (the target variable), the date, type, and condition of the sale, the building class, the zoning class, the lot area in square feet, the street acccess type and road proximity, the property configuration, shape, and slope, neighborhood quality, dwelling type and style, the property's condition and material quality, the construction and remodeling date, infos about the property's roof, foundation, electrical system, exterior, masonry veneer, garage, pool, porch, deck, and basement, the property's heating, fireplace, and air conditioning quality, infos about the rooms like bathrooms, bedroom, and kitchen, and finally how functional the home is overall.\n",
    "\n",
    "![Numerical and categorical data sample](images/categorical_and_numerical_data_close.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of the features are numerical values, some of them are continuous like dollars, area, distance, and some of them are discrete like quantity, year, month.\n",
    "\n",
    "One point of attention here is that `MSSubClass` number (for example) is just an identifier, as per the [dataset's author description](https://github.com/adrifmonte/house-prices/blob/master/data/data_description.txt), and should be transformed into categorical data.\n",
    "\n",
    "There are also lots of categorical features, some of them are nominal like types, styles, and shapes, and they should be one-hot-encoded, but there are also some categorical values that are ordinal, that may be ordered and transformed into numberical values, like class, grades, quality. Some of them may have their values scaled.\n",
    "\n",
    "One point of attention here is that some ordinal features such as `LandSlope` and `GarageFinish` (for example) are ordinal values, as per the [dataset's author description](https://github.com/adrifmonte/house-prices/blob/master/data/data_description.txt), and should be transformed into numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Missing values analysis, and outliers analysis](images/missing_values_plus_outliers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values and outliers\n",
    "\n",
    "The Missing values chart shows us, in addition to the `count` value in the description tables above, that some features have missing values which should be handled considering each own particularities and percentage of missing values.\n",
    "\n",
    "After handling missing values, it was noticed some feature outliers (some of these features are transformed data). These outliers datapoints (see Feature outliers table) will influence the regression model in a bad way, and they should be dropped to avoid that. It was considered an outlier if the feature datapoint was beyond the edge quartiles, the same technique applied in the Clustering Project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization\n",
    "\n",
    "[comment]: # (In this section, you will need to provide some form of visualization that summarizes or extracts a relevant characteristic or feature about the data. The visualization should adequately support the data being used. Discuss why this visualization was chosen and how it is relevant. Questions to ask yourself when writing this section:)\n",
    "\n",
    "[comment]: # (Have you visualized a relevant characteristic or feature about the dataset or input data?)\n",
    "[comment]: # (Is the visualization thoroughly analyzed and discussed?)\n",
    "[comment]: # (If a plot is provided, are the axes, title, and datum clearly defined?)\n",
    "\n",
    "One of the main selected features was the NeighborhoodMeanPrice, which was created by the mean SalePrice of each Neighborhood. Before starting the project, I asked my partner:\n",
    "\n",
    "> What criterias do you think that best defines the price of a house?\n",
    "\n",
    "And she said:\n",
    "\n",
    "> Hmm... The house size, quality, and the neighborhood, I guess.\n",
    "\n",
    "Well, that actually makes sense, and as expected, the Neighborhood indeed represents a lot of how much a house should be valued.\n",
    "\n",
    "Note that before plotting, the dataset was sorted by its `SalePrice`, and the chart also followed with an ascending neighborhood sale price, with a few exceptions. Those exceptions are probably due to some outlier prices in some neighborhoods, but one thing is unquestionable: the neighborhood dictates a lot of the house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/neighborhood_wide.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques\n",
    "\n",
    "[comment]: # (In this section, you will need to discuss the algorithms and techniques you intend to use for solving the problem. You should justify the use of each one based on the characteristics of the problem and the problem domain. Questions to ask yourself when writing this section:)\n",
    "\n",
    "[comment]: # (Are the algorithms you will use, including any default variables/parameters in the project clearly defined?)\n",
    "[comment]: # (Are the techniques to be used thoroughly discussed and justified?)\n",
    "[comment]: # (Is it made clear how the input data or datasets will be handled by the algorithms and techniques chosen?)\n",
    "\n",
    "- Data Processing Techniques\n",
    "\n",
    "I intended to properly process data, and guarantee that no garbage is coming as input, so considerable effort by using different methods and techniques should be put to handling missing values <sup>[1](https://towardsdatascience.com/handling-missing-values-in-machine-learning-part-1-dda69d4f88ca),[2](https://www.analyticsindiamag.com/5-ways-handle-missing-values-machine-learning-datasets/),[3](https://stats.stackexchange.com/a/391643/176988),[4](https://towardsdatascience.com/handling-missing-data-for-a-beginner-6d6f5ea53436),[5](http://www.statsmodels.org/dev/imputation.html),[6](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4#cbda)</sup> (dropping features, mean imputation, mode imputation, new type imputation, fill with zero, imputation by deriving from other features), making type conversion <sup>[1](https://stackoverflow.com/q/41335718/1064325),[2](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4#bc9d),[3](https://www.datacamp.com/community/tutorials/categorical-data)\n",
    "[4](https://pbpython.com/categorical-encoding.html),[5](https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159),[6](https://pt.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv),[7](https://towardsdatascience.com/why-you-should-try-mean-encoding-17057262cd0),[8](https://www.datacamp.com/community/tutorials/encoding-methodologies)</sup> (type casting, mean encoding, one hot encoding, mapping values), handling outliers (dropping rows), aggregating features (sum, weighted sum, subtraction, age calculation, mean, division, weighted boosting), scaling features <sup>[1](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4#d078),[2](https://www.codecademy.com/articles/normalization),[3](https://machinelearningmastery.com/feature-selection-machine-learning-python/),[4](https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e)</sup> (log scaling, mean normalization, min-max scaling), [selecting features](https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e) (problem relevance, feature variance, feature correlation, target correlation, feature importance).\n",
    "\n",
    "- Clustering Algorithms and Techniques\n",
    "\n",
    "Before clustering, I intended to perform a PCA transformation to avoid the [curse of dimensionality](https://stats.stackexchange.com/q/256172/176988) (and review the Clustering with different number of components).\n",
    "\n",
    "In this moment I intended to try different clustering models with different algorithms (such as [K-Means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), [GMM](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html), and [Random Forest](https://nishanthu.github.io/articles/ClusteringUsingRandomForest.html)), evaluating them with [`silhouette_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) to have a feeling on the performance of each algorithm (do that for each number of clusters in a reasonable range of values).\n",
    "\n",
    "With the selected model and number of clusters through the [`silhouette_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html), predict the cluster of each house in both train and test data.\n",
    "\n",
    "- Regression Clustering Algorithms and Techniques\n",
    "\n",
    "For each cluster found, I intended to train models with different algorithms ([Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), [Stochastic Gradient Descent](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html), [XGBoost Regressor](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor)) and benchmark each of them with [K-Fold cross-validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) using [`mean_squared_log_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html) to measure and validate the performance of each algorithm, making sure that they generalize well.\n",
    "\n",
    "Also perform hyperparameters grid search with both [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html), and if applicable do some quick round of EDA and/or data processing, and rerun the benchmark with the changes in order to find the best model for each cluster. Reprocess everything after having inserted an additional 10% of noisy data to assert the robustness of the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "[comment]: # (In this section, you will need to provide a clearly defined benchmark result or threshold for comparing across performances obtained by your solution. The reasoning behind the benchmark {in the case where it is not an established result} should be discussed. Questions to ask yourself when writing this section:)\n",
    "\n",
    "[comment]: # (Has some result or value been provided that acts as a benchmark for measuring performance?)\n",
    "[comment]: # (Is it clear how this result or value was obtained {whether by data or by hypothesis}?)\n",
    "\n",
    "After handling all missing values, let's set a naive baseline.\n",
    "\n",
    "- Naive Prediction (Simple Math)\n",
    "\n",
    "A simple math Naive Predictor was built to have a minimum base benchmark to beat with a more sophisticated method. This naive prediction can be calculated by the Lot Area of the predicting house multiplied by the dataset's mean Sale Price per Lot Area square feet, which can be calculated dividing the dataset's mean Sale Price, by the dataset's mean Lot Area.\n",
    "\n",
    "The Naive Predictor should be naïve, yet reasonable, and as previously described, it can be achieved with simple math, as exemplified in the following code:\n",
    "\n",
    "```\n",
    "naive_train_X = aligned_encoded_train_data.drop(columns=['Id', 'SalePrice'])\n",
    "naive_train_y = aligned_encoded_train_data['SalePrice']\n",
    "\n",
    "mean_sale_price_per_lot_area = naive_train_y.mean() / naive_train_X['LotArea'].mean()\n",
    "\n",
    "def naive_fit(X, y):\n",
    "    mean_sale_price_per_lot_area = y.mean() / X['LotArea'].mean()\n",
    "    return mean_sale_price_per_lot_area\n",
    "\n",
    "def naive_predict(X, mean_sale_price_per_lot_area):\n",
    "    return X['LotArea'] * mean_sale_price_per_lot_area\n",
    "```\n",
    "\n",
    "This predictor was not only trained and tested with KFold but was also submitted to the Kaggle's competition to understand how it performs by itself:\n",
    "\n",
    "|Train score|Test score|Kaggle score|\n",
    "|---|---|---|\n",
    "|0.5034265623764532|0.522732597478724||\n",
    "|0.5299006769793394|0.5109719880215422|0.55909|\n",
    "\n",
    "- Naive Prediction (Linear Regression)\n",
    "\n",
    "In order to also use an algorithm in the naive prediction phase, a simple Linear Regression was made with all the features. It trained nicely, but was very overfitted: the test and Kaggle scores were very high:\n",
    "\n",
    "|Train score|Test score|Kaggle score|\n",
    "|---|---|---|\n",
    "|0.12314555019521473|1.8408330078556492||\n",
    "|0.13708035922180273|1.5926004067904753|1.38515|\n",
    "\n",
    "It probably overfitted due to the curse of dimensionality: there were too many features! So, it was reasonable to give a chance of making a Linear Regression with one feature, `LotArea` - the same used in the \"simple math\" Naive Prediction. Let's see how it performed:\n",
    "\n",
    "|Train score|Test score|Kaggle score|\n",
    "|---|---|---|\n",
    "|0.3633472894098743|0.3927671323761031||\n",
    "|0.40727444683178804|0.3795332401680835|0.42035|\n",
    "\n",
    "Nice, it didn't overfitted as the first try, and it scored better than the simple math predictor.\n",
    "\n",
    "Let's use `~0.4` as the Naive Prediction score.\n",
    "\n",
    "- Kaggle Competition\n",
    "\n",
    "Analysing the [Kaggle's House Prices Leaderboard data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/leaderboard), after removing some last mile submission outliers, I intended to have as the main benchmark to achieve a submitted score under `0.138`, and then under the first quartile of `0.120`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "[comment]: # (_approx. 3-5 pages_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "[comment]: # (In this section, all of your preprocessing steps will need to be clearly documented, if any were necessary. From the previous section, any of the abnormalities or characteristics that you identified about the dataset will be addressed and corrected here. Questions to ask yourself when writing this section:)\n",
    "\n",
    "[comment]: # (If the algorithms chosen require preprocessing steps like feature selection or feature transformations, have they been properly documented?)\n",
    "[comment]: # (Based on the Data Exploration section, if there were abnormalities or characteristics that needed to be addressed, have they been properly corrected?)\n",
    "[comment]: # (If no preprocessing is needed, has it been made clear why?)\n",
    "\n",
    "#### Handling missing values <sup>[1](https://towardsdatascience.com/handling-missing-values-in-machine-learning-part-1-dda69d4f88ca),[2](https://www.analyticsindiamag.com/5-ways-handle-missing-values-machine-learning-datasets/),[3](https://stats.stackexchange.com/a/391643/176988),[4](https://towardsdatascience.com/handling-missing-data-for-a-beginner-6d6f5ea53436),[5](http://www.statsmodels.org/dev/imputation.html),[6](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4#cbda)</sup>\n",
    "\n",
    "- Dropping columns\n",
    "\n",
    "When there are too many missing data in a feature, it is reasonable to drop its corresponding column. It was done for `PoolQC`, `MiscFeature`, `Alley`, and `Fence` features.\n",
    "\n",
    "- New type imputation\n",
    "\n",
    "When there are some missing categorical data and they mean none or empty or other or no value, it is fine to create a new category to represent them, specially when they have their own importance when evaluating their correspondence with the target value. It was done for `FireplaceQu`, `GarageType`, `GarageFinish`, `GarageQual`, `GarageCond`, `BsmtExposure`, `BsmtFinType2`, `BsmtFinType1`, `BsmtCond`, `BsmtQual`, `MasVnrType`, `Exterior1st`, and `Exterior2nd`.\n",
    "\n",
    "The following helper facilitates making that kind of imputation:\n",
    "\n",
    "```\n",
    "def fillna_with(feature, new_type = 'N/A'):\n",
    "    train_data[feature].fillna(new_type, inplace=True)\n",
    "    test_data[feature].fillna(new_type, inplace=True)\n",
    "```\n",
    "\n",
    "- Fill with zero\n",
    "\n",
    "When there are some missing numerical data and they mean none or empty or no value, and zero could represent it nicely, fill them with zero, specially when they they are missing just in the test data and you don't want to look into them. It was done for `MasVnrArea`, `BsmtFullBath`, `BsmtHalfBath`, `BsmtFinSF1`, `BsmtFinSF2`, `BsmtUnfSF`, `TotalBsmtSF`, `GarageCars`, and `GarageArea`.\n",
    "\n",
    "- Mode imputation\n",
    "\n",
    "When there are some missing data and they can't mean none or zero, specially when they are missing just in the test data and you don't want to look into them, it may be reasonable to fill them with their most frequent value, since it's the most probable feature value. It was done for `Electrical`, `MSZoning`, `Utilities`, `Functional`, `KitchenQual`, and `SaleType`.\n",
    "\n",
    "The following helper facilitates making that kind of imputation when there is missing values in the testing data:\n",
    "\n",
    "```\n",
    "def fillna_test_with(feature, value):\n",
    "    test_data.loc[test_data[feature].isnull(), feature] = value\n",
    "\n",
    "def fillna_test_with_train_mode(feature):\n",
    "    feature_train_mode = train_data[feature].describe().top\n",
    "    fillna_test_with(feature, feature_train_mode)\n",
    "```\n",
    "\n",
    "- Imputation by deriving from other features\n",
    "\n",
    "When a feature can be derived from other one, it is makes sense to use some math to establish or simulate a resonable value that would still get the essence of that datapoint. It was done with `LotFrontage`, and `GarageYrBlt`.\n",
    "\n",
    "For example, the following shows how `LotFrontage` was derived from `LotArea`, even when it is missing in the test data:\n",
    "\n",
    "```\n",
    "# LotFrontage can be derived from LotArea, impute based on a calculated mean Lot depth\n",
    "lot_depth = train_data['LotArea'] / train_data['LotFrontage']\n",
    "lot_depth_mean = lot_depth.mean()\n",
    "\n",
    "# Populates a possible probable missing LotFrontage, by dividing its LotArea by the (generated) LotDepth mean\n",
    "train_lot_frontage_nan_indexes = train_data['LotFrontage'].isnull()\n",
    "train_lot_frontage_missing_imputation = train_data[train_lot_frontage_nan_indexes]['LotArea'] / lot_depth_mean\n",
    "train_data.loc[train_lot_frontage_nan_indexes, 'LotFrontage'] = train_lot_frontage_missing_imputation\n",
    "\n",
    "test_lot_frontage_nan_indexes = test_data['LotFrontage'].isnull()\n",
    "test_lot_frontage_missing_imputation = test_data[test_lot_frontage_nan_indexes]['LotArea'] / lot_depth_mean\n",
    "test_data.loc[test_lot_frontage_nan_indexes, 'LotFrontage'] = test_lot_frontage_missing_imputation\n",
    "```\n",
    "\n",
    "#### Making type conversion <sup>[1](https://stackoverflow.com/q/41335718/1064325),[2](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4#bc9d),[3](https://www.datacamp.com/community/tutorials/categorical-data),[4](https://pbpython.com/categorical-encoding.html),[5](https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159),[6](https://pt.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv),[7](https://towardsdatascience.com/why-you-should-try-mean-encoding-17057262cd0),[8](https://www.datacamp.com/community/tutorials/encoding-methodologies)</sup>\n",
    "\n",
    "- Type casting\n",
    "\n",
    "When the data is not represented in its best type, converting it is the way to go. In our case, `MSSubClass` and `MoSold` were converted from `int64` to `object` (python dataframe's `string`). Also, `LotFrontage`, `LotDepth` (feature created), `MasVnrArea`, and `GarageYrBlt` were converted from `float64` to `int64` for convenience.\n",
    "\n",
    "- One hot encoding (dummy values)\n",
    "\n",
    "When a categorical feature is nominal, we create new binary columns, each column represents one category and their dummy values will tell if that datapoint is true (1) or false (0) for that column. This process is called \"one hot encoding\", and it adds lots of features, increasing the dataset's dimensionality. This process was done to `MSSubClass`, `MSZoning`, `LandContour`, `Street`, `LotConfig`, `BldgType`, `HouseStyle`, `RoofStyle`, `RoofMatl`, `MoSold`, `MasVnrType`, `Foundation`, `Heating`, `Electrical`, `PavedDrive`, `Condition1`, `Condition2`, `Exterior1st`, and `Exterior2nd`.\n",
    "\n",
    "One intesting issue is that, when the set of a nominal feature differs in train and test data, it may end up with different shapes for the training data and the testing data. To make them have the same shape, there's an `align` method from panda's `DataFrame` which will create missing columns in both sides, with a given value, in this case, we want to fill with `0`, since the dummy value would indeed be that:\n",
    "\n",
    "```\n",
    "# ... one hot encoding\n",
    "\n",
    "print('After one hot encoding shapes:')\n",
    "display(encoded_train_data_with_dummies.shape)\n",
    "display(encoded_test_data_with_dummies.shape)\n",
    "\n",
    "# https://stackoverflow.com/a/47240395/1064325\n",
    "aligned_encoded_train_data, aligned_encoded_test_data = encoded_train_data_with_dummies.align(\n",
    "    encoded_test_data_with_dummies, join='outer', axis=1, fill_value=0\n",
    ")\n",
    "\n",
    "print('After one hot encoding alignment shapes:')\n",
    "display(aligned_encoded_train_data.shape)\n",
    "display(aligned_encoded_test_data.shape)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "> Before one hot encoding shapes:\n",
    "> (1460, 76)\n",
    "> (1459, 75)\n",
    ">\n",
    "> After one hot encoding shapes:\n",
    "> (1460, 199)\n",
    "> (1459, 187)\n",
    ">\n",
    "> After one hot encoding alignment shapes:\n",
    "> (1460, 201)\n",
    "> (1459, 201)\n",
    "\n",
    "Also note that the `SalePrice` have also been (unwantedly) aligned, so we may drop it.\n",
    "\n",
    "- Mapping values\n",
    "\n",
    "When a categorical feature is ordinal, it makes sense to map their categories to ordered integers, so the algorithms will work better. The following are the features that were mapped to ordered numbers: `LotShape`, `LandSlope`, `Utilities`, `ExterQual`, `ExterCond`, `BsmtQual`, `BsmtCond`, `BsmtExposure`, `HeatingQC`, `CentralAir`, `KitchenQual`, `Functional`, `FireplaceQu`, `GarageFinish`, `GarageQual`, `GarageCond`, `BsmtFinType1`, and `BsmtFinType2`.\n",
    "\n",
    "- Target mean encoding\n",
    "\n",
    "When the data is nominal, but it looks like that it could actually be ordered, but not logically as a grade value, demanding some extra effort, one good option is to use the target mean encoding. The `Neighborhood` and the `GarageType`, for example, may be ordered if we consider their average target value, and that's what the target mean encoding technique is for.\n",
    "\n",
    "```\n",
    "encoded_train_data = train_data.copy()\n",
    "encoded_test_data = test_data.copy()\n",
    "\n",
    "def mean_encode(data, t_data, feature, encoded_feature_name):\n",
    "    categories = data[feature].unique()\n",
    "    \n",
    "    # target mean encode\n",
    "    for category in categories:\n",
    "        category_indexes = data[feature] == category\n",
    "        t_category_indexes = t_data[feature] == category\n",
    "        category_target_mean = data[category_indexes]['SalePrice'].mean()\n",
    "        data.loc[category_indexes, feature] = category_target_mean\n",
    "        t_data.loc[t_category_indexes, feature] = category_target_mean\n",
    "    \n",
    "    # rename\n",
    "    data[encoded_feature_name] = data[feature].astype('float64').round(0).astype('int64')\n",
    "    t_data[encoded_feature_name] = t_data[feature].astype('float64').round(0).astype('int64')\n",
    "    \n",
    "    # drop old column name\n",
    "    data.drop(columns=[feature], inplace=True)\n",
    "    t_data.drop(columns=[feature], inplace=True)\n",
    "\n",
    "# target-mean-encode Neighborhood and GarageType into NeighborhoodMeanPrice and GarageTypeMeanPrice\n",
    "mean_encode(encoded_train_data, encoded_test_data, 'Neighborhood', 'NeighborhoodMeanPrice')\n",
    "mean_encode(encoded_train_data, encoded_test_data, 'GarageType', 'GarageTypeMeanPrice')\n",
    "```\n",
    "\n",
    "#### Handling outliers\n",
    "\n",
    "- Dropping rows\n",
    "\n",
    "When we find outliers, it may impact our model negativelly. Hence, those datapoints can reasonably be dropped if the test and validation results shows an improvement without them, which in general does. In this case, after removing outliers from the training data, we've gone from 1460 datapoints to 1028, after analysing datapoints that are outliers in more than one feature for the non-binary features.\n",
    "\n",
    "#### Aggregating features\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Sum\n",
    "\n",
    "```\n",
    "good_data['FloorSF'] = good_data['1stFlrSF'] + good_data['2ndFlrSF']\n",
    "test_data['FloorSF'] = test_data['1stFlrSF'] + test_data['2ndFlrSF']\n",
    "```\n",
    "\n",
    "- Weighted sum\n",
    "\n",
    "```\n",
    "good_data['BsmtBathrooms'] = good_data['BsmtFullBath'] + (good_data['BsmtHalfBath'] * 0.5)\n",
    "test_data['BsmtBathrooms'] = test_data['BsmtFullBath'] + (test_data['BsmtHalfBath'] * 0.5)\n",
    "```\n",
    "\n",
    "- Subtraction\n",
    "\n",
    "```\n",
    "good_data['NetBuildingMeanPrice'] = good_data['NeighborhoodMeanPrice']\n",
    "                                    - good_data['GarageTypeMeanPrice']\n",
    "                                    - good_data['MiscVal']\n",
    "test_data['NetBuildingMeanPrice'] = test_data['NeighborhoodMeanPrice']\n",
    "                                    - test_data['GarageTypeMeanPrice']\n",
    "                                    - test_data['MiscVal']\n",
    "```\n",
    "\n",
    "- Age calculation\n",
    "\n",
    "```\n",
    "good_data['LotAge'] = 2019 - good_data['YearBuilt']\n",
    "test_data['LotAge'] = 2019 - test_data['YearBuilt']\n",
    "```\n",
    "\n",
    "- Mean\n",
    "\n",
    "```\n",
    "good_data['OverallRating'] = (good_data['OverallQual'] + good_data['OverallCond']) / 2\n",
    "test_data['OverallRating'] = (test_data['OverallQual'] + test_data['OverallCond']) / 2\n",
    "```\n",
    "\n",
    "- Division\n",
    "\n",
    "```\n",
    "train_data['LotDepth'] = train_data['LotArea'] / train_data['LotFrontage']\n",
    "test_data['LotDepth'] = test_data['LotArea'] / test_data['LotFrontage']\n",
    "```\n",
    "\n",
    "- Weighted boosting\n",
    "\n",
    "```\n",
    "good_data['WeightedNeighborhoodMeanPrice'] = good_data['NeighborhoodMeanPrice']\n",
    "                                            * (1. + (.1 / good_data['LotAge']))\n",
    "test_data['WeightedNeighborhoodMeanPrice'] = test_data['NeighborhoodMeanPrice']\n",
    "                                            * (1. + (.1 / test_data['LotAge']))\n",
    "```\n",
    "\n",
    "#### Scaling features <sup>[1](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4#d078),[2](https://www.codecademy.com/articles/normalization),[3](https://machinelearningmastery.com/feature-selection-machine-learning-python/),[4](https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e)</sup>\n",
    "\n",
    "- Mean normalization\n",
    "\n",
    "Initially I had applied the mean normalization for currency and area related features, but I discovered that it performed much better when not normalizing them. That finding was actually what have put me with `0.19446` in Kaggle, and beat the Naive Predictor.\n",
    "\n",
    "- Min-max scaling\n",
    "\n",
    "As a good practice, I also have applied scaling to some features related to time, grade, and quantity, using the min-max scaling technique. Some created features that felt into time, grade or quantity, also had their values applied with min-max scaling.\n",
    "\n",
    "- Log scaling\n",
    "\n",
    "I read about log scaling, and tried it, since the mean normalization failed in giving good results, but it also have failed, so I just dropped this attempt, but it's still worth mentioning and documenting it.\n",
    "\n",
    "#### Selecting features <sup>[1](https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e)</sup>\n",
    "\n",
    "In order to avoid the curse of dimensionality, feature selection is a very important step to have a good model. Let's see different approaches that were used in this Capstone Project:\n",
    "\n",
    "- Problem relevance\n",
    "\n",
    "Some features may be irrelevant, such as datapoint IDs or non-related features. In this case, I decided to drop `SaleType`, and `SaleCondition`, since I was interested in discovering the house prices based in the house data, and not in the sale data.\n",
    "\n",
    "- Feature variance\n",
    "\n",
    "Features that have very low variance shouldn't help much with supervised learning, because they have little or nothing to tell us. Since we have a lot of features, let's drop the features which variance are lower than `0.2`:\n",
    "\n",
    "```\n",
    "def run_var_analysis():\n",
    "    features = reduced_var_train_data.drop(columns=['Id', 'SalePrice'])\n",
    "\n",
    "    var_feats = features.var()\n",
    "    var_feats_ordered = var_feats.sort_values(ascending=True)\n",
    "    return var_feats_ordered[var_feats_ordered < 0.2]\n",
    "\n",
    "low_variance_feats = run_var_analysis()\n",
    "\n",
    "reduced_var_train_data.drop(columns=low_variance_feats.index, inplace=True)\n",
    "reduced_var_test_data.drop(columns=low_variance_feats.index, inplace=True)\n",
    "```\n",
    "\n",
    "- Feature correlation\n",
    "\n",
    "Features that are very correlated to some other feature also won't help much, since the other feature already gives us the same essence of that feature, which means that when we find a highly correlated (above `0.8`) tuple of features, we can drop one of that features without losing much.\n",
    "\n",
    "```\n",
    "# find highly correlated features\n",
    "def run_corr_analysis():\n",
    "    features = reduced_train_data.drop(columns=['Id', 'SalePrice'])\n",
    "\n",
    "    corr = features.corr().abs()\n",
    "    corr[corr == 1] = 0\n",
    "    corr_cols = corr.max().sort_values(ascending=False)\n",
    "    return corr_cols[corr_cols > 0.8]\n",
    "\n",
    "corr_results = run_corr_analysis()\n",
    "\n",
    "corr_result_index = 0\n",
    "for corr_result in corr_results.index:\n",
    "    # drop just one feature of each \"tuple\"\n",
    "    if (corr_result_index % 2 == 1):\n",
    "        reduced_train_data.drop(columns=[corr_result], inplace=True)\n",
    "        reduced_test_data.drop(columns=[corr_result], inplace=True)\n",
    "    \n",
    "    corr_result_index = corr_result_index + 1\n",
    "```\n",
    "\n",
    "- Target correlation\n",
    "\n",
    "Features that have little correlation (under `0.2`) with the target may also be dropped because they wouldn't help the supervised learning model to predict the target value.\n",
    "\n",
    "```\n",
    "def run_target_corr_analysis():\n",
    "    features = reduced_train_data.drop(columns=['Id'])\n",
    "\n",
    "    corr = features.corr().abs()\n",
    "    target_corr = corr['SalePrice'].sort_values(ascending=True)\n",
    "    return target_corr[target_corr < 0.2]\n",
    "\n",
    "lowly_correlated_feats = run_target_corr_analysis()\n",
    "\n",
    "reduced_train_data.drop(columns=lowly_correlated_feats.index, inplace=True)\n",
    "reduced_test_data.drop(columns=lowly_correlated_feats.index, inplace=True)\n",
    "```\n",
    "\n",
    "- Feature importance\n",
    "\n",
    "Finally, from the remaining features, we may drop features with low importance. Some regressors outputs a feature importance array telling which features were more significant to make the regression prediction itself. In this case I've used `RandomForestRegressor` to calculate the feature importance, and dropped features which importance were lower than `0.02`.\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor(n_jobs=-1, random_state=42, n_estimators=50)\n",
    "\n",
    "feature_selected_train_data = reduced_train_data.copy()\n",
    "feature_selected_test_data = reduced_test_data.copy()\n",
    "\n",
    "def run_importance_analysis():\n",
    "    features = feature_selected_train_data.drop(columns=['Id', 'SalePrice'])\n",
    "    \n",
    "    rfr_importance_analyser = rfr.fit(features, feature_selected_train_data['SalePrice'])\n",
    "    return features.columns[rfr_importance_analyser.feature_importances_ < 0.02]\n",
    "\n",
    "low_importance_feats = run_importance_analysis()\n",
    "\n",
    "feature_selected_train_data.drop(columns=low_importance_feats, inplace=True)\n",
    "feature_selected_test_data.drop(columns=low_importance_feats, inplace=True)\n",
    "```\n",
    "\n",
    "#### Feature transformation\n",
    "\n",
    "- Dimensionality reduction\n",
    "\n",
    "Before clustering, I intended to perform a PCA transformation, but even with different number of components, I couldn't get a better result than when I wasn't using PCA, so I ended up dropping it from the Project itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "[comment]: # (In this section, the process for which metrics, algorithms, and techniques that you implemented for the given data will need to be clearly documented. It should be abundantly clear how the implementation was carried out, and discussion should be made regarding any complications that occurred during this process. Questions to ask yourself when writing this section:)\n",
    "\n",
    "[comment]: # (Is it made clear how the algorithms and techniques were implemented with the given datasets or input data?)\n",
    "[comment]: # (Were there any complications with the original metrics or techniques that required changing prior to acquiring a solution?)\n",
    "[comment]: # (Was there any part of the coding process {e.g., writing complicated functions} that should be documented?)\n",
    "\n",
    "- Metrics\n",
    "\n",
    "During this Capstone Project's development, there were three main important things to note regarding metrics.\n",
    "The first is that I had to handle negative input values (replacing with 0) before passing to `mean_squared_log_error` in a custom scorer, otherwise it would throw an error. The negative results was being probably intensified because of the second issue: I forgot to set `greater_is_better` to `False` when creating the scorer with [`make_scorer`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) for the grid search scoring.\n",
    "\n",
    "The last one is that at first I was using `mean_squared_log_error` alone, but then, at some late point I [realized](http://mkhalusova.github.io/blog/2019/04/17/ml-model-evaluation-metrics-p3#rmsle) that I missed to also apply the squared root of the `mean_squared_log_error`. Hopefully, the trained model results didn't change after that fix, but it became much better to properly compare results with Kaggle submission.\n",
    "\n",
    "Finally, with everything fixed, the metric creation ended up with the following code:\n",
    "\n",
    "```\n",
    "def custom_scorer(y, y_pred):\n",
    "    y_pred[y_pred < 0] = 0\n",
    "    return np.sqrt(mean_squared_log_error(y, y_pred))\n",
    "\n",
    "scorer = make_scorer(custom_scorer, greater_is_better=False)\n",
    "```\n",
    "\n",
    "- Clustering Algorithm\n",
    "\n",
    "I was advised in the Capstone Proposal review that doing Clustering before Regression wouldn't be significant for this dataset, so I almost dropped all of this. In the end I was trying so hard lots of things, that I ended up trying KMeans. It really haven't made a significant improvement, but it improved my final result, so I kept it. Since it wasn't the main objective, I didn't went through different clustering techniques.\n",
    "\n",
    "```\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "clusters_range = range(2, 11)\n",
    "scores = []\n",
    "\n",
    "for n_clusters in clusters_range:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_jobs=-1, random_state=42)\n",
    "    result = kmeans.fit_predict(feature_selected_train_data.drop(columns=['Id', 'SalePrice']))\n",
    "    \n",
    "    curr_score = silhouette_score(feature_selected_train_data, result)\n",
    "    scores.append(curr_score)\n",
    "    \n",
    "cluster_scores_df = pd.DataFrame({ 'scores': scores, 'n_clusters': clusters_range })\n",
    "\n",
    "best_score = cluster_scores_df['scores'].max()\n",
    "n_clusters = cluster_scores_df[cluster_scores_df['scores'] == best_score]['n_clusters'].values[0]\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_jobs=-1, random_state=42)\n",
    "result = kmeans.fit_predict(feature_selected_train_data.drop(columns=['Id', 'SalePrice']))\n",
    "test_result = kmeans.predict(feature_selected_test_data.drop(columns=['Id']))\n",
    "\n",
    "feature_selected_train_data['Cluster'] = result\n",
    "feature_selected_test_data['Cluster'] = test_result\n",
    "```\n",
    "\n",
    "Actually, it was just in the clustering attempt (attempt \\#28) that I could beat myself in validation with a score of `0.18706` at Kaggle (I had firstly beaten the Naive Predictor in my attempt \\#6 by not normalizing the currency features, when reached `0.19446` in the competition). Despite it's not much, it helped.\n",
    "\n",
    "One interesting finding that have helped to perform (very slightly) better was to use one extra cluster than the best scored one (from `0.18706` to `0.18616`, and later from `0.18163` to `0.18140`).\n",
    "\n",
    "I also tried to \"recluster\" one cluster that was being badly scored, but it ended up having an even worse score, so I dropped that attempt as well. Lesson learned.\n",
    "\n",
    "- Regression\n",
    "\n",
    "I ended up not trying XGBoost Regressor, neither LightGBM Regressor because I needed to upgrade my python environment to install them, and at that point I didn't have enough time for that. In the other hand, I did tried lots of other regressors available in `sklearn`, such as: `LinearRegression`, `RandomForestRegressor`, `GradientBoostingRegressor`, `Lasso`, `Ridge`, and `MLPRegressor` (Multi-layer Perceptron Regressor), and they did pretty well.\n",
    "\n",
    "I did have used `KFold` and `GridSearchCV`, but didn't used `RandomizedSearchCV` as I planned at first, but I think it could still do the job. I've created two helper functions (`kfold` and `tune_model`) that made it easy to implement the tuning for all those 6 regressors (and others that I tried but dismissed due to poor performance).\n",
    "\n",
    "```\n",
    "from sklearn.metrics import mean_squared_log_error, make_scorer\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "def kfold(model, name, cluster, train_X, train_y):\n",
    "    kf = KFold(n_splits=2, random_state=42, shuffle=False)\n",
    "    \n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "\n",
    "    for kf_chunks in kf.split(train_X):\n",
    "        train_index = kf_chunks[0]\n",
    "        test_index = kf_chunks[1]\n",
    "\n",
    "        X_train = train_X.iloc[train_index]\n",
    "        X_test = train_X.iloc[test_index]\n",
    "        y_train = train_y.iloc[train_index]\n",
    "        y_test = train_y.iloc[test_index]\n",
    "\n",
    "        model_predictor = model.fit(X_train, y_train)\n",
    "        y_train_predicted = model_predictor.predict(X_train)\n",
    "        y_test_predicted = model_predictor.predict(X_test)\n",
    "\n",
    "        y_train_predicted[y_train_predicted < 0] = 0\n",
    "        y_test_predicted[y_test_predicted < 0] = 0\n",
    "\n",
    "        train_score = np.sqrt(mean_squared_log_error(y_train, y_train_predicted))\n",
    "        test_score = np.sqrt(mean_squared_log_error(y_test, y_test_predicted))\n",
    "        \n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "\n",
    "        print('[{}][{}] Current fold train performance'.format(name, cluster), train_score)\n",
    "        print('[{}][{}] Current fold test performance'.format(name, cluster), test_score)\n",
    "        \n",
    "    return train_scores, test_scores\n",
    "\n",
    "def tune_model(Model, name, default_params, cv_params):    \n",
    "    best_params = []\n",
    "    merged_params_arr = []\n",
    "    model_train_scores = []\n",
    "    model_test_scores = []\n",
    "    mean_train_scores = []\n",
    "    mean_test_scores = []\n",
    "\n",
    "    for cluster in range(n_clusters):\n",
    "        current_cluster_filter = feature_selected_train_data['Cluster'] == cluster\n",
    "\n",
    "        gs_train_X = feature_selected_train_data[current_cluster_filter].drop(columns=['Id', 'SalePrice', 'Cluster'])\n",
    "        gs_train_y = feature_selected_train_data[current_cluster_filter]['SalePrice']\n",
    "\n",
    "        gs = GridSearchCV(Model(**default_params), cv_params, n_jobs=-1, cv=2, scoring=gs_scorer)\n",
    "\n",
    "        gs_predictor = gs.fit(gs_train_X, gs_train_y)\n",
    "\n",
    "        best_params.append(gs_predictor.best_params_)\n",
    "        \n",
    "        merged_params = dict()\n",
    "        merged_params.update(default_params)\n",
    "        merged_params.update(gs_predictor.best_params_)\n",
    "        merged_params_arr.append(merged_params)\n",
    "        \n",
    "        train_scores, test_scores = kfold(Model(**merged_params), name, cluster, gs_train_X, gs_train_y)\n",
    "        \n",
    "        model_train_scores.append(train_scores)\n",
    "        model_test_scores.append(test_scores)\n",
    "        \n",
    "        mean_train_scores.append(np.mean(train_scores))\n",
    "        mean_test_scores.append(np.mean(test_scores))\n",
    "\n",
    "    model_arr = np.full((1, n_clusters), name, object)[0]\n",
    "    return pd.DataFrame({\n",
    "        'cluster': range(n_clusters),\n",
    "        'params_cv': best_params,\n",
    "        'params_full': merged_params_arr,\n",
    "        'model': model_arr,\n",
    "        'train_scores': model_train_scores,\n",
    "        'test_scores': model_test_scores,\n",
    "        'train_score': mean_train_scores,\n",
    "        'test_score': mean_test_scores,\n",
    "    })\n",
    "```\n",
    "\n",
    "After implementing all those regressors tuning, I've selected the best regressor for each cluster, and rebuilt them in order to submit the final mixed value. It was my Kaggle submission attempt \\#41, my last resort, and it indeed improved my mark from `0.18228` to `0.18163` by using multiple mixed custom models for each cluster (Lasso and Ridge have got the best results), instead of just Linear Regressions for each cluster.\n",
    "\n",
    "```\n",
    "# join the performance of all dataframes so we can select the best one easier\n",
    "all_df = linear_regression_df.copy()\n",
    "# all_df = all_df.append(random_forest_df.copy()) # overfit\n",
    "# all_df = all_df.append(gradient_boost_df.copy()) # overfit\n",
    "all_df = all_df.append(lasso_df.copy())\n",
    "all_df = all_df.append(ridge_df.copy())\n",
    "all_df = all_df.append(mlpr_df.copy())\n",
    "\n",
    "# initialize the selected models for each cluster\n",
    "selected_models = pd.DataFrame({\n",
    "    'cluster': [],\n",
    "    'model': [],\n",
    "    'params_full': [],\n",
    "    'test_score': []\n",
    "})\n",
    "\n",
    "# for each cluster find the model that performed better\n",
    "for cluster in range(n_clusters):\n",
    "    cluster_best_models = all_df[all_df['cluster'] == cluster]\n",
    "                            .sort_values(by='test_score', ascending=True)\n",
    "    cluster_best_model = cluster_best_models.head(1)[\n",
    "                            ['cluster', 'model', 'params_full', 'test_score']\n",
    "                         ]\n",
    "    selected_models = selected_models.append(cluster_best_model)\n",
    "\n",
    "selected_models['cluster'] = selected_models['cluster'].astype(int)\n",
    "selected_models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement\n",
    "\n",
    "[comment]: # (In this section, you will need to discuss the process of improvement you made upon the algorithms and techniques you used in your implementation. For example, adjusting parameters for certain models to acquire improved solutions would fall under the refinement category. Your initial and final solutions should be reported, as well as any significant intermediate results as necessary. Questions to ask yourself when writing this section:)\n",
    "\n",
    "[comment]: # (Has an initial solution been found and clearly reported?)\n",
    "[comment]: # (Is the process of improvement clearly documented, such as what techniques were used?)\n",
    "[comment]: # (Are intermediate and final solutions clearly reported as the process is improved?)\n",
    "\n",
    "In addition to the aforementioned refinements, let me summarize the refinement journey:\n",
    "\n",
    "#### What have helped:\n",
    "\n",
    "- Kaggle Attempt \\#06: scored 0.19446\n",
    "\n",
    "Removed currency features normalization (used Random Forest Regressor with `max_depth=10, n_estimators=100, min_samples_leaf=3, min_samples_split=2`).\n",
    "\n",
    "- Kaggle Attempt \\#28: scored 0.18706\n",
    "\n",
    "Clustered Linear Regression (used Linear Regressions with `fit_intercept=True, normalize=True`).\n",
    "\n",
    "- Kaggle Attempt \\#30: scored 0.18616\n",
    "\n",
    "Clustered (with one extra cluster) Linear Regression (used Linear Regressions with `fit_intercept=True, normalize=True`).\n",
    "\n",
    "- Kaggle Attempt \\#39: scored 0.18228\n",
    "\n",
    "Created more Features (used Linear Regressions with `fit_intercept=True, normalize=True`).\n",
    "\n",
    "- Kaggle Attempt \\#41: scored 0.18163\n",
    "\n",
    "Mixed algorithms, no extra cluster (Lasso and Ridge with the params defined in their dynamic dataframes)\n",
    "\n",
    "- Kaggle Attempt \\#42: scored 0.18140\n",
    "\n",
    "Mixed algorithms, with one extra cluster (Lasso and Ridge with the params defined in their dynamic dataframes)\n",
    "\n",
    "#### What haven't helped:\n",
    "\n",
    "- Relaxing feature selection to have more features (curse of dimensionality)\n",
    "- Making feature normalization or log scaling to currency features\n",
    "- Some algorithms performed really bad: `SGDRegressor`, for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Validation\n",
    "\n",
    "To get the dynamic selected model, rebuild and submit the final result for validation in Kaggle, with those mixed models, I've done the following:\n",
    "\n",
    "```\n",
    "# since the selected models are dynamic,\n",
    "# we need to dynamically build the model in this phase\n",
    "# the params will come from `params_full` column of the `selected_models` df\n",
    "def build_model(model, params):\n",
    "    if (model == 'Linear Regression'):\n",
    "        return LinearRegression(**params)\n",
    "    if (model == 'Lasso'):\n",
    "        return Lasso(**params)\n",
    "    if (model == 'Ridge'):\n",
    "        return Ridge(**params)\n",
    "    # other models may be added here if necessary\n",
    "\n",
    "results = []\n",
    "ids = []\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    current_cluster_filter = feature_selected_train_data['Cluster'] == cluster\n",
    "    \n",
    "    # create X and y for training\n",
    "    train_X = feature_selected_train_data[current_cluster_filter]\n",
    "                    .drop(columns=['Id', 'SalePrice', 'Cluster'])\n",
    "    train_y = feature_selected_train_data[current_cluster_filter]['SalePrice']\n",
    "    \n",
    "    # create X and y for validation\n",
    "    current_cluster_filter = feature_selected_test_data['Cluster'] == cluster\n",
    "    predicting_X = feature_selected_test_data[current_cluster_filter]\n",
    "                    .drop(columns=['Id', 'Cluster'])\n",
    "    predicting_ids = feature_selected_test_data[current_cluster_filter]['Id']\n",
    "    \n",
    "    # create the model dynamically for this cluster\n",
    "    selected_model_df = selected_models[selected_models['cluster'] == cluster]\n",
    "    model = build_model(\n",
    "        selected_model_df['model'].values[0],\n",
    "        selected_model_df['params_full'].values[0]\n",
    "    )\n",
    "    \n",
    "    # fit and predict\n",
    "    model = model.fit(train_X, train_y)\n",
    "    prediction = model.predict(predicting_X)\n",
    "    \n",
    "    # save results\n",
    "    results = np.concatenate((results, prediction))\n",
    "    ids = np.concatenate((ids, predicting_ids))\n",
    "\n",
    "# build final csv to submit on Kaggle\n",
    "kaggle_sb_df = pd.DataFrame({ 'SalePrice': results, 'Id': ids })\n",
    "kaggle_sb_df['Id'] = kaggle_sb_df['Id'].astype(int)\n",
    "kaggle_sb_df = kaggle_sb_df.sort_values(by='Id')\n",
    "kaggle_sb_df.to_csv('data/kaggle_submission_42.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification\n",
    "\n",
    "Hopefully, as described in my journey, I could build indeed a better model than the Naive Predictor score (`0.42035`), and then beat myself 5 times more:\n",
    "\n",
    "Kaggle Attempt #06: scored 0.19446\n",
    "Kaggle Attempt #28: scored 0.18706\n",
    "Kaggle Attempt #30: scored 0.18616\n",
    "Kaggle Attempt #39: scored 0.18228\n",
    "Kaggle Attempt #41: scored 0.18163\n",
    "Kaggle Attempt #42: scored 0.18140\n",
    "\n",
    "I actually couldn't get even close to the Kaggle's mark (`~0.13`) I intended to reach at first, the competition was more challenging than I thought, which made me strieve to find the best I could, learning and trying lots of things! Hopefully, the Udacity reviewer that revised my Capstone Proposal commented that the benchmarking was more about comparing final results with the Naive Predictor than comparing with other real world solutions.\n",
    "\n",
    "Anyway, I think it is a reasonable solution, did pretty well solving the house prices prediction problem when compared with the Naive Predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free-Form Visualization\n",
    "\n",
    "A special characteristic of this work, as described above, is the\n",
    "\n",
    "|cluster|model|params_full|test_score|\n",
    "|---|---|---|---|\n",
    "|0|Lasso|{u'normalize': False, u'selection': u'random',...|0.107076|\n",
    "|1|Lasso|{u'normalize': True, u'selection': u'random', ...|0.169924|\n",
    "|2|Ridge|{u'normalize': False, u'fit_intercept': True, ...|0.085253|\n",
    "|3|Lasso|{u'normalize': True, u'selection': u'cyclic', ...|0.114486|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "In addition to the aforementioned difficulties, the most difficult part was trying to beat myself, personally organize eveything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement\n",
    "\n",
    "Could use some blending stuff that I saw in some Kaggles Kernels, that's what I can think of right now, the other improvements I've already tried."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
